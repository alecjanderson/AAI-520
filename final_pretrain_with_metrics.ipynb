{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "e5b4c0a0",
      "metadata": {
        "id": "e5b4c0a0"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import torch\n",
        "import spacy\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from contractions import contractions_dict\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "import nltk\n",
        "from rouge_score import rouge_scorer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install contractions\n",
        "#!pip install transformers"
      ],
      "metadata": {
        "id": "qMQuWM1uUlUn"
      },
      "id": "qMQuWM1uUlUn",
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Vl47RnsDeqoJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f45006a-2933-44a4-864f-17497560dde3"
      },
      "id": "Vl47RnsDeqoJ",
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\")"
      ],
      "metadata": {
        "id": "uiHgp01icQo4"
      },
      "id": "uiHgp01icQo4",
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "6b288a46",
      "metadata": {
        "id": "6b288a46"
      },
      "outputs": [],
      "source": [
        "# 1. Model Definition & set padding\n",
        "model_name = 'microsoft/dialogpt-medium'\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name, padding_side='left')\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "2GdIDHUhWmeX",
      "metadata": {
        "id": "2GdIDHUhWmeX"
      },
      "outputs": [],
      "source": [
        "movie_lines_path = '/content/drive/MyDrive/AAI510/movie_lines.txt'\n",
        "movie_conversations_path = '/content/drive/MyDrive/AAI510/movie_conversations.txt'"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OSHH6qYfb5Hq"
      },
      "id": "OSHH6qYfb5Hq",
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "0bac7002",
      "metadata": {
        "id": "0bac7002"
      },
      "outputs": [],
      "source": [
        "lines = open(movie_lines_path, encoding='utf-8', errors='ignore').read().split('\\n')\n",
        "convers = open(movie_conversations_path, encoding='utf-8', errors='ignore').read().split('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e5efb0e",
      "metadata": {
        "id": "8e5efb0e"
      },
      "source": [
        "**Text Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "9ab8de4d",
      "metadata": {
        "id": "9ab8de4d"
      },
      "outputs": [],
      "source": [
        "class TextPreprocessor:\n",
        "    def __init__(self, max_len=13):\n",
        "        self.max_len = max_len\n",
        "        self._compile_regex()\n",
        "\n",
        "    def _compile_regex(self):\n",
        "        # Contractions\n",
        "        self.compiled_patterns = {re.compile(pattern): repl for pattern, repl in contractions_dict.items()}\n",
        "        # Retain important punctuation\n",
        "        self.clean_punctuations = re.compile(r'[^a-zA-Z0-9?.!,Â¿]')\n",
        "\n",
        "    def clean_text(self, txt):\n",
        "        txt = txt.lower()\n",
        "        for pattern, repl in self.compiled_patterns.items():\n",
        "            txt = pattern.sub(repl, txt)\n",
        "        txt = self.clean_punctuations.sub(' ', txt)\n",
        "        return txt.strip()\n",
        "\n",
        "    def preprocess_data(self, convers, lines):\n",
        "        exchange = [conver.split(' +++$+++ ')[-1][1:-1].replace(\"'\", \" \").replace(\",\", \"\").split() for conver in convers]\n",
        "        diag = {line.split(' +++$+++ ')[0]: line.split(' +++$+++ ')[-1] for line in lines}\n",
        "        questions, answers = self._extract_questions_answers(exchange, diag)\n",
        "        return questions, answers\n",
        "\n",
        "    def _extract_questions_answers(self, exchange, diag):\n",
        "        questions, answers = [], []\n",
        "        for conver in exchange:\n",
        "            for i in range(len(conver) - 1):\n",
        "                questions.append(diag.get(conver[i], ''))\n",
        "                answers.append(diag.get(conver[i + 1], ''))\n",
        "        sorted_ques = [q for q in questions if len(q.split()) < self.max_len]\n",
        "        sorted_ans = [a for q, a in zip(questions, answers) if len(q.split()) < self.max_len]\n",
        "        return sorted_ques, sorted_ans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "ff0e1d9a",
      "metadata": {
        "id": "ff0e1d9a"
      },
      "outputs": [],
      "source": [
        "max_len = 30\n",
        "max_seq_len = 60\n",
        "\n",
        "preprocessor = TextPreprocessor(max_len=max_len)\n",
        "sorted_ques, sorted_ans = preprocessor.preprocess_data(convers, lines)\n",
        "clean_ques = [preprocessor.clean_text(q) for q in sorted_ques]\n",
        "clean_ans = [preprocessor.clean_text(a) for a in sorted_ans]\n",
        "\n",
        "# Trimming answers and lists\n",
        "clean_ans = [' '.join(ans.split()[:max_len - 2]) for ans in clean_ans]\n",
        "clean_ans = clean_ans[:3000]\n",
        "clean_ques = clean_ques[:3000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "79e1a023",
      "metadata": {
        "id": "79e1a023",
        "outputId": "5e9eab1f-5784-4a0c-d7c4-7d00c191944e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.', \"Well, I thought we'd start with pronunciation, if that's okay with you.\", 'Not the hacking and gagging and spitting part.  Please.', \"You're asking me out.  That's so cute. What's your name again?\", \"No, no, it's my fault -- we didn't have a proper introduction ---\"]\n"
          ]
        }
      ],
      "source": [
        "# Before cleaning\n",
        "print(sorted_ques[:5])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sorted_ans[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOCyCrHRds7t",
        "outputId": "bb196b1f-5d85-4f4d-9bdf-cf27438db28e"
      },
      "id": "VOCyCrHRds7t",
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"Well, I thought we'd start with pronunciation, if that's okay with you.\", 'Not the hacking and gagging and spitting part.  Please.', \"Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\", 'Forget it.', 'Cameron.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "a7eb9bdd",
      "metadata": {
        "id": "a7eb9bdd",
        "outputId": "7981bb39-4043-427d-cf04-757a6fd5311d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['can we make this quick?  roxanne korrine and andrew barrett are having an incredibly horrendous public break  up on the quad.  again.', 'well, i thought we would start with pronunciation, if that is okay with you.', 'not the hacking and gagging and spitting part.  please.', 'you are asking me out.  that is so cute. what is your name again?', 'no, no, it is my fault    we did not have a proper introduction']\n",
            "['well, i thought we would start with pronunciation, if that is okay with you.', 'not the hacking and gagging and spitting part. please.', 'okay... then how bout we try out some french cuisine. saturday? night?', 'forget it.', 'cameron.']\n"
          ]
        }
      ],
      "source": [
        "# After cleaning\n",
        "print(clean_ques[:5])\n",
        "print(clean_ans[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "0f8735cf",
      "metadata": {
        "id": "0f8735cf"
      },
      "outputs": [],
      "source": [
        "class DialogDataset(Dataset):\n",
        "    def __init__(self, clean_ques, clean_ans, tokenizer, max_length):\n",
        "        self.clean_ques = clean_ques\n",
        "        self.clean_ans = clean_ans\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.clean_ques)\n",
        "\n",
        "    @staticmethod\n",
        "    def collate_fn(batch):\n",
        "        inputs, targets, input_masks, target_masks, input_token_type_ids, target_token_type_ids = zip(*batch)\n",
        "        inputs = pad_sequence(inputs, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "        targets = pad_sequence(targets, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "        input_masks = pad_sequence(input_masks, batch_first=True, padding_value=0)\n",
        "        target_masks = pad_sequence(target_masks, batch_first=True, padding_value=0)\n",
        "        input_token_type_ids = pad_sequence(input_token_type_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "        target_token_type_ids = pad_sequence(target_token_type_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "        return inputs, targets, input_masks, target_masks, input_token_type_ids, target_token_type_ids\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        question = self.clean_ques[idx]\n",
        "        answer = self.clean_ans[idx]\n",
        "\n",
        "        question_tokenized = self.tokenizer(question, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt', return_token_type_ids=True)\n",
        "        answer_tokenized = self.tokenizer(answer, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt', return_token_type_ids=True)\n",
        "\n",
        "        return (\n",
        "            question_tokenized['input_ids'].squeeze(),\n",
        "            answer_tokenized['input_ids'].squeeze(),\n",
        "            question_tokenized['attention_mask'].squeeze(),\n",
        "            answer_tokenized['attention_mask'].squeeze(),\n",
        "            question_tokenized.get('token_type_ids', torch.tensor([])).squeeze(),  # Return empty tensor if not present\n",
        "            answer_tokenized.get('token_type_ids', torch.tensor([])).squeeze()\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = DialogDataset(clean_ques, clean_ans, tokenizer, max_seq_len)\n",
        "\n",
        "# Create DataLoader\n",
        "batch_size = 32\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=DialogDataset.collate_fn)\n",
        "\n",
        "# Get a sample of 5 questions and answers pairs\n",
        "sample_size = 5\n",
        "sample_questions = []\n",
        "sample_answers = []\n",
        "\n",
        "for batch in dataloader:\n",
        "    inputs, targets, _, _, _, _ = batch\n",
        "    for i in range(sample_size):\n",
        "        question = tokenizer.decode(inputs[i], skip_special_tokens=True)\n",
        "        answer = tokenizer.decode(targets[i], skip_special_tokens=True)\n",
        "        sample_questions.append(question)\n",
        "        sample_answers.append(answer)\n",
        "    if len(sample_questions) >= sample_size:\n",
        "        break\n",
        "\n",
        "# Print\n",
        "for i in range(sample_size):\n",
        "    print(f\"Question {i+1}: {sample_questions[i]}\")\n",
        "    print(f\"Answer {i+1}: {sample_answers[i]}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4emeNIl3fmEz",
        "outputId": "6f839ade-3056-4676-cc34-1edd79f2e50f"
      },
      "id": "4emeNIl3fmEz",
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question 1: what does he say?\n",
            "Answer 1: he asks when he can come to visit you. he left his address.\n",
            "\n",
            "Question 2: i missed you.\n",
            "Answer 2: it says here you exposed yourself to a group of freshmen girls.\n",
            "\n",
            "Question 3: about jack? no. have you checked out the tax bill on your father s farm lately?\n",
            "Answer 3: i know he is due for the last two years. i was thinking of paying it when the insurance comes in.\n",
            "\n",
            "Question 4: hmm.\n",
            "Answer 4: the only difference between me and that clemens on tv is luck, shit luck.\n",
            "\n",
            "Question 5: yeah...\n",
            "Answer 5: that means they are going to give me the key to the pool so i can lock up when i m done.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "f3f980d5",
      "metadata": {
        "id": "f3f980d5"
      },
      "outputs": [],
      "source": [
        "# Dataloader, split into train and val\n",
        "BATCH_SIZE = 80\n",
        "\n",
        "# Split\n",
        "train_ques, val_ques, train_ans, val_ans = train_test_split(clean_ques, clean_ans, test_size=0.2, random_state=42)\n",
        "\n",
        "# Dataset\n",
        "train_dataset = DialogDataset(train_ques, train_ans, tokenizer, max_seq_len)\n",
        "val_dataset = DialogDataset(val_ques, val_ans, tokenizer, max_seq_len)\n",
        "\n",
        "# Dataloader\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=DialogDataset.collate_fn)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=DialogDataset.collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "f17d2d49",
      "metadata": {
        "id": "f17d2d49",
        "outputId": "49431aba-a805-478d-ae54-feb29d4aa454",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 1024)\n",
              "    (wpe): Embedding(1024, 1024)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-23): 24 x GPT2Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ],
      "source": [
        "EPOCHS = 20\n",
        "LR = 5e-5\n",
        "\n",
        "# Early stopping\n",
        "best_val_loss = float('inf')\n",
        "no_improve = 0\n",
        "patience = 4\n",
        "\n",
        "# Model\n",
        "optimizer = AdamW(model.parameters(), lr=LR)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=1, verbose=True)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "af3af2b5",
      "metadata": {
        "id": "af3af2b5",
        "outputId": "a6e18ef4-c23f-49be-d35e-823d6b078551",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Training Loss: 1.9883749683698018, Validation Loss: 1.7925520092248917\n",
            "Epoch 1, Training Loss: 1.7379539092381795, Validation Loss: 1.68807552754879\n",
            "Epoch 2, Training Loss: 1.9338445782661438, Validation Loss: 2.1962030827999115\n",
            "Epoch 00004: reducing learning rate of group 0 to 5.0000e-06.\n",
            "Epoch 3, Training Loss: 1.881255598862966, Validation Loss: 1.7272789776325226\n",
            "Epoch 4, Training Loss: 1.738474138577779, Validation Loss: 1.7283093929290771\n",
            "Epoch 00006: reducing learning rate of group 0 to 5.0000e-07.\n",
            "Epoch 5, Training Loss: 1.760448149840037, Validation Loss: 1.7552176862955093\n",
            "Early stopping!\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    #Training\n",
        "    for batch_num, (batch_inputs, batch_targets, input_masks, target_masks, input_token_type_ids, target_token_type_ids) in enumerate(train_dataloader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        batch_inputs, batch_targets = batch_inputs.to(device), batch_targets.to(device)\n",
        "        input_masks = input_masks.to(device)\n",
        "\n",
        "        outputs = model(batch_inputs, attention_mask=input_masks)\n",
        "        loss = loss_fn(outputs.logits.view(-1, outputs.logits.size(-1)), batch_targets.view(-1))\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch_num != 0 and batch_num % 30 == 0:\n",
        "            print(f\"Epoch {epoch}, Batch {batch_num}, Loss: {loss.item()}\")\n",
        "\n",
        "    # compute the average loss for the epoch\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "    scheduler.step(avg_train_loss)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_inputs, batch_targets, _, val_input_masks, _, _ in val_dataloader:\n",
        "            batch_inputs, batch_targets = batch_inputs.to(device), batch_targets.to(device)\n",
        "            val_input_masks = val_input_masks.to(device)\n",
        "\n",
        "            outputs = model(batch_inputs, attention_mask=val_input_masks)\n",
        "            loss = loss_fn(outputs.logits.view(-1, outputs.logits.size(-1)), batch_targets.view(-1))\n",
        "            total_val_loss += loss.item()\n",
        "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
        "    print(f\"Epoch {epoch}, Training Loss: {avg_train_loss}, Validation Loss: {avg_val_loss}\")\n",
        "\n",
        "    # Early Stopping\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        no_improve = 0\n",
        "    else:\n",
        "        no_improve += 1\n",
        "    if no_improve == patience:\n",
        "        print(\"Early stopping!\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "4a527bbc",
      "metadata": {
        "id": "4a527bbc"
      },
      "outputs": [],
      "source": [
        "def get_response(input_text, model, tokenizer, num_beams=7, max_length=80, temperature=0.5, top_k=50, no_repeat_ngram_size=2):\n",
        "    model.eval()\n",
        "\n",
        "    encoded_input = tokenizer.encode(input_text, return_tensors='pt', truncation=True)\n",
        "    input_length = encoded_input.shape[1]\n",
        "\n",
        "    if input_length + 10 > max_length:\n",
        "        max_length = input_length + 10\n",
        "\n",
        "    input_tensor = encoded_input.to(device)\n",
        "    attention_mask = torch.ones(input_tensor.shape).to(device)\n",
        "\n",
        "    # Beam search\n",
        "    output_ids = model.generate(\n",
        "        input_tensor,\n",
        "        max_length=max_length,\n",
        "        num_beams=num_beams,\n",
        "        early_stopping=True,\n",
        "        attention_mask=attention_mask,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "        no_repeat_ngram_size=no_repeat_ngram_size\n",
        "    )\n",
        "\n",
        "\n",
        "    response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    response = post_process(response)\n",
        "\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "db22ae23",
      "metadata": {
        "id": "db22ae23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54eb5322-82d9-4d1e-99ec-d3e3e8718eff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello in from the other side.\n"
          ]
        }
      ],
      "source": [
        "input_query = \"Hello\"\n",
        "response = get_response(input_query, model, tokenizer)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"punkt\")\n",
        "\n",
        "# Reference responses\n",
        "reference_responses = [\n",
        "    \"women like that have a way of turning professionals into amateurs.\",\n",
        "    \"wade, that was not smarch. going out right after the man s father in law shot himself. let it go. call it a favor to me.\",\n",
        "    \"it is scurvy s. his date got convicted. where did you get the dress?\",\n",
        "    \"not bad.\",\n",
        "    \"yes, it is.\"\n",
        "]\n",
        "\n",
        "# Clean the reference responses\n",
        "preprocessor = TextPreprocessor(max_len=30)\n",
        "reference_responses = [preprocessor.clean_text(response) for response in reference_responses]\n",
        "\n",
        "input_queries = [\n",
        "    \"hey. i m a professional.\",\n",
        "    \"he say anything about the summons i tried to give him? sonofabitch would not accept it.\",\n",
        "    \"how did you get a tux at the last minute?\",\n",
        "    \"how are you feeling, fernando?\",\n",
        "    \"too bad about frank, is not it?\"\n",
        "]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQxGsgPii0Ee",
        "outputId": "8e5c44c9-b78e-4b7d-fdd3-dd5872a2ae9a"
      },
      "id": "JQxGsgPii0Ee",
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lRgSxEXjjQJY"
      },
      "id": "lRgSxEXjjQJY",
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_responses = []\n",
        "\n",
        "for query in input_queries:\n",
        "    response = get_response(query, model, tokenizer)\n",
        "    model_responses.append(response)\n",
        "\n",
        "model_responses = [preprocessor.clean_text(response) for response in model_responses]\n",
        "\n",
        "# BLEU Calculation\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "bleu_scores = [sentence_bleu([ref.split()], model.split()) for ref, model in zip(reference_responses, model_responses)]\n",
        "average_bleu = sum(bleu_scores) / len(bleu_scores)\n",
        "\n",
        "# ROUGE Calculation\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "rouge1_scores = []\n",
        "rouge2_scores = []\n",
        "rougeL_scores = []\n",
        "\n",
        "for ref, model in zip(reference_responses, model_responses):\n",
        "    scores = scorer.score(ref, model)\n",
        "    rouge1_scores.append(scores['rouge1'].fmeasure)\n",
        "    rouge2_scores.append(scores['rouge2'].fmeasure)\n",
        "    rougeL_scores.append(scores['rougeL'].fmeasure)\n",
        "\n",
        "average_rouge1 = sum(rouge1_scores) / len(rouge1_scores)\n",
        "average_rouge2 = sum(rouge2_scores) / len(rouge2_scores)\n",
        "average_rougeL = sum(rougeL_scores) / len(rougeL_scores)\n",
        "\n",
        "print(\"Average BLEU Score:\", average_bleu)\n",
        "print(\"Average ROUGE-1 Score:\", average_rouge1)\n",
        "print(\"Average ROUGE-2 Score:\", average_rouge2)\n",
        "print(\"Average ROUGE-L Score:\", average_rougeL)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OiY2sBRZWrS",
        "outputId": "a6c04f4c-96a5-4908-834f-9b25cc4974dd"
      },
      "id": "5OiY2sBRZWrS",
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average BLEU Score: 5.3160331857435324e-79\n",
            "Average ROUGE-1 Score: 0.2238759689922481\n",
            "Average ROUGE-2 Score: 0.03636363636363637\n",
            "Average ROUGE-L Score: 0.16527131782945736\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}