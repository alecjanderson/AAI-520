{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "e5b4c0a0",
      "metadata": {
        "id": "e5b4c0a0"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import torch\n",
        "import spacy\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from contractions import contractions_dict\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install contractions\n",
        "#!pip install transformers"
      ],
      "metadata": {
        "id": "qMQuWM1uUlUn"
      },
      "id": "qMQuWM1uUlUn",
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Vl47RnsDeqoJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca9e44a0-dacb-483e-cf38-7ee942506033"
      },
      "id": "Vl47RnsDeqoJ",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\")"
      ],
      "metadata": {
        "id": "uiHgp01icQo4"
      },
      "id": "uiHgp01icQo4",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "6b288a46",
      "metadata": {
        "id": "6b288a46"
      },
      "outputs": [],
      "source": [
        "# 1. Model Definition & set padding\n",
        "model_name = 'microsoft/dialogpt-medium'\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name, padding_side='left')\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "2GdIDHUhWmeX",
      "metadata": {
        "id": "2GdIDHUhWmeX"
      },
      "outputs": [],
      "source": [
        "movie_lines_path = '/content/drive/MyDrive/AAI510/movie_lines.txt'\n",
        "movie_conversations_path = '/content/drive/MyDrive/AAI510/movie_conversations.txt'"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OSHH6qYfb5Hq"
      },
      "id": "OSHH6qYfb5Hq",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "0bac7002",
      "metadata": {
        "id": "0bac7002"
      },
      "outputs": [],
      "source": [
        "lines = open(movie_lines_path, encoding='utf-8', errors='ignore').read().split('\\n')\n",
        "convers = open(movie_conversations_path, encoding='utf-8', errors='ignore').read().split('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e5efb0e",
      "metadata": {
        "id": "8e5efb0e"
      },
      "source": [
        "**Text Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "9ab8de4d",
      "metadata": {
        "id": "9ab8de4d"
      },
      "outputs": [],
      "source": [
        "class TextPreprocessor:\n",
        "    def __init__(self, max_len=13):\n",
        "        self.max_len = max_len\n",
        "        self._compile_regex()\n",
        "\n",
        "    def _compile_regex(self):\n",
        "        # Contractions\n",
        "        self.compiled_patterns = {re.compile(pattern): repl for pattern, repl in contractions_dict.items()}\n",
        "        # Retain important punctuation\n",
        "        self.clean_punctuations = re.compile(r'[^a-zA-Z0-9?.!,Â¿]')\n",
        "\n",
        "    def clean_text(self, txt):\n",
        "        txt = txt.lower()\n",
        "        for pattern, repl in self.compiled_patterns.items():\n",
        "            txt = pattern.sub(repl, txt)\n",
        "        txt = self.clean_punctuations.sub(' ', txt)\n",
        "        return txt.strip()\n",
        "\n",
        "    def preprocess_data(self, convers, lines):\n",
        "        exchange = [conver.split(' +++$+++ ')[-1][1:-1].replace(\"'\", \" \").replace(\",\", \"\").split() for conver in convers]\n",
        "        diag = {line.split(' +++$+++ ')[0]: line.split(' +++$+++ ')[-1] for line in lines}\n",
        "        questions, answers = self._extract_questions_answers(exchange, diag)\n",
        "        return questions, answers\n",
        "\n",
        "    def _extract_questions_answers(self, exchange, diag):\n",
        "        questions, answers = [], []\n",
        "        for conver in exchange:\n",
        "            for i in range(len(conver) - 1):\n",
        "                questions.append(diag.get(conver[i], ''))\n",
        "                answers.append(diag.get(conver[i + 1], ''))\n",
        "        sorted_ques = [q for q in questions if len(q.split()) < self.max_len]\n",
        "        sorted_ans = [a for q, a in zip(questions, answers) if len(q.split()) < self.max_len]\n",
        "        return sorted_ques, sorted_ans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "ff0e1d9a",
      "metadata": {
        "id": "ff0e1d9a"
      },
      "outputs": [],
      "source": [
        "max_len = 30\n",
        "max_seq_len = 60\n",
        "\n",
        "preprocessor = TextPreprocessor(max_len=max_len)\n",
        "sorted_ques, sorted_ans = preprocessor.preprocess_data(convers, lines)\n",
        "clean_ques = [preprocessor.clean_text(q) for q in sorted_ques]\n",
        "clean_ans = [preprocessor.clean_text(a) for a in sorted_ans]\n",
        "\n",
        "# Trimming answers and lists\n",
        "clean_ans = [' '.join(ans.split()[:max_len - 2]) for ans in clean_ans]\n",
        "clean_ans = clean_ans[:3000]\n",
        "clean_ques = clean_ques[:3000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "79e1a023",
      "metadata": {
        "id": "79e1a023",
        "outputId": "edbfdf11-be63-4875-f4c7-599d579462c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.', \"Well, I thought we'd start with pronunciation, if that's okay with you.\", 'Not the hacking and gagging and spitting part.  Please.', \"You're asking me out.  That's so cute. What's your name again?\", \"No, no, it's my fault -- we didn't have a proper introduction ---\"]\n"
          ]
        }
      ],
      "source": [
        "# Before cleaning\n",
        "print(sorted_ques[:5])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sorted_ans[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOCyCrHRds7t",
        "outputId": "e45e4102-7b00-45bd-ed39-5059e8d51fd0"
      },
      "id": "VOCyCrHRds7t",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"Well, I thought we'd start with pronunciation, if that's okay with you.\", 'Not the hacking and gagging and spitting part.  Please.', \"Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\", 'Forget it.', 'Cameron.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "a7eb9bdd",
      "metadata": {
        "id": "a7eb9bdd",
        "outputId": "ff728d5e-1497-4c95-8862-d328d4afd0b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['can we make this quick?  roxanne korrine and andrew barrett are having an incredibly horrendous public break  up on the quad.  again.', 'well, i thought we would start with pronunciation, if that is okay with you.', 'not the hacking and gagging and spitting part.  please.', 'you are asking me out.  that is so cute. what is your name again?', 'no, no, it is my fault    we did not have a proper introduction']\n",
            "['well, i thought we would start with pronunciation, if that is okay with you.', 'not the hacking and gagging and spitting part. please.', 'okay... then how bout we try out some french cuisine. saturday? night?', 'forget it.', 'cameron.']\n"
          ]
        }
      ],
      "source": [
        "# After cleaning\n",
        "print(clean_ques[:5])\n",
        "print(clean_ans[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "0f8735cf",
      "metadata": {
        "id": "0f8735cf"
      },
      "outputs": [],
      "source": [
        "class DialogDataset(Dataset):\n",
        "    def __init__(self, clean_ques, clean_ans, tokenizer, max_length):\n",
        "        self.clean_ques = clean_ques\n",
        "        self.clean_ans = clean_ans\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.clean_ques)\n",
        "\n",
        "    @staticmethod\n",
        "    def collate_fn(batch):\n",
        "        inputs, targets, input_masks, target_masks, input_token_type_ids, target_token_type_ids = zip(*batch)\n",
        "        inputs = pad_sequence(inputs, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "        targets = pad_sequence(targets, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "        input_masks = pad_sequence(input_masks, batch_first=True, padding_value=0)\n",
        "        target_masks = pad_sequence(target_masks, batch_first=True, padding_value=0)\n",
        "        input_token_type_ids = pad_sequence(input_token_type_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "        target_token_type_ids = pad_sequence(target_token_type_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "        return inputs, targets, input_masks, target_masks, input_token_type_ids, target_token_type_ids\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        question = self.clean_ques[idx]\n",
        "        answer = self.clean_ans[idx]\n",
        "\n",
        "        question_tokenized = self.tokenizer(question, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt', return_token_type_ids=True)\n",
        "        answer_tokenized = self.tokenizer(answer, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt', return_token_type_ids=True)\n",
        "\n",
        "        return (\n",
        "            question_tokenized['input_ids'].squeeze(),\n",
        "            answer_tokenized['input_ids'].squeeze(),\n",
        "            question_tokenized['attention_mask'].squeeze(),\n",
        "            answer_tokenized['attention_mask'].squeeze(),\n",
        "            question_tokenized.get('token_type_ids', torch.tensor([])).squeeze(),  # Return empty tensor if not present\n",
        "            answer_tokenized.get('token_type_ids', torch.tensor([])).squeeze()\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the DialogDataset class\n",
        "dataset = DialogDataset(clean_ques, clean_ans, tokenizer, max_seq_len)\n",
        "\n",
        "# Create a DataLoader\n",
        "batch_size = 32  # You can adjust the batch size as needed\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=DialogDataset.collate_fn)\n",
        "\n",
        "# Get a sample of 5 questions and answers pairs\n",
        "sample_size = 5\n",
        "sample_questions = []\n",
        "sample_answers = []\n",
        "\n",
        "for batch in dataloader:\n",
        "    inputs, targets, _, _, _, _ = batch\n",
        "    for i in range(sample_size):\n",
        "        question = tokenizer.decode(inputs[i], skip_special_tokens=True)\n",
        "        answer = tokenizer.decode(targets[i], skip_special_tokens=True)\n",
        "        sample_questions.append(question)\n",
        "        sample_answers.append(answer)\n",
        "    if len(sample_questions) >= sample_size:\n",
        "        break\n",
        "\n",
        "# Print the sample questions and answers\n",
        "for i in range(sample_size):\n",
        "    print(f\"Question {i+1}: {sample_questions[i]}\")\n",
        "    print(f\"Answer {i+1}: {sample_answers[i]}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4emeNIl3fmEz",
        "outputId": "7694f6a2-f18f-462f-8767-1da155494abb"
      },
      "id": "4emeNIl3fmEz",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question 1: hey.  i m a professional.\n",
            "Answer 1: women like that have a way of turning professionals into amateurs.\n",
            "\n",
            "Question 2: he say anything about the summons i tried to give him? sonofabitch would not accept it.\n",
            "Answer 2: wade, that was not smarch. going out right after the man s father in law shot himself. let it go. call it a favor to me.\n",
            "\n",
            "Question 3: how did you get a tux at the last minute?\n",
            "Answer 3: it is scurvy s. his date got convicted. where did you get the dress?\n",
            "\n",
            "Question 4: how are you feeling, fernando?\n",
            "Answer 4: not bad.\n",
            "\n",
            "Question 5: too bad about frank, is not it?\n",
            "Answer 5: yes, it is.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "f3f980d5",
      "metadata": {
        "id": "f3f980d5"
      },
      "outputs": [],
      "source": [
        "# Dataloader, split into train and val\n",
        "BATCH_SIZE = 80\n",
        "\n",
        "# Split\n",
        "train_ques, val_ques, train_ans, val_ans = train_test_split(clean_ques, clean_ans, test_size=0.2, random_state=42)\n",
        "\n",
        "# Dataset\n",
        "train_dataset = DialogDataset(train_ques, train_ans, tokenizer, max_seq_len)\n",
        "val_dataset = DialogDataset(val_ques, val_ans, tokenizer, max_seq_len)\n",
        "\n",
        "# Dataloader\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=DialogDataset.collate_fn)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=DialogDataset.collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "f17d2d49",
      "metadata": {
        "id": "f17d2d49",
        "outputId": "72460de0-baea-42e4-d8a4-406935f5435e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 1024)\n",
              "    (wpe): Embedding(1024, 1024)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-23): 24 x GPT2Block(\n",
              "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "EPOCHS = 20\n",
        "LR = 5e-5\n",
        "\n",
        "# Early stopping\n",
        "best_val_loss = float('inf')\n",
        "no_improve = 0\n",
        "patience = 4\n",
        "\n",
        "# Model\n",
        "optimizer = AdamW(model.parameters(), lr=LR)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=1, verbose=True)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "af3af2b5",
      "metadata": {
        "id": "af3af2b5",
        "outputId": "4bb280b4-7826-4bcd-ea11-7f3339096b3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Training Loss: 2.000652964909871, Validation Loss: 1.7533817887306213\n",
            "Epoch 1, Training Loss: 1.8127119898796082, Validation Loss: 2.0494414567947388\n",
            "Epoch 2, Training Loss: 1.8477030952771505, Validation Loss: 1.681943342089653\n",
            "Epoch 3, Training Loss: 1.7290360450744628, Validation Loss: 1.616935595870018\n",
            "Epoch 4, Training Loss: 1.7052722016970316, Validation Loss: 1.7933958619832993\n",
            "Epoch 5, Training Loss: 2.146157467365265, Validation Loss: 2.0902405083179474\n",
            "Epoch 00007: reducing learning rate of group 0 to 5.0000e-06.\n",
            "Epoch 6, Training Loss: 1.780916945139567, Validation Loss: 1.7805142849683762\n",
            "Epoch 7, Training Loss: 1.6827802220980326, Validation Loss: 1.7615855485200882\n",
            "Early stopping!\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    #Training\n",
        "    for batch_num, (batch_inputs, batch_targets, input_masks, target_masks, input_token_type_ids, target_token_type_ids) in enumerate(train_dataloader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        batch_inputs, batch_targets = batch_inputs.to(device), batch_targets.to(device)\n",
        "        input_masks = input_masks.to(device)\n",
        "\n",
        "        outputs = model(batch_inputs, attention_mask=input_masks)\n",
        "        loss = loss_fn(outputs.logits.view(-1, outputs.logits.size(-1)), batch_targets.view(-1))\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch_num != 0 and batch_num % 30 == 0:\n",
        "            print(f\"Epoch {epoch}, Batch {batch_num}, Loss: {loss.item()}\")\n",
        "\n",
        "    # compute the average loss for the epoch\n",
        "    avg_train_loss = total_loss / len(train_dataloader)\n",
        "    scheduler.step(avg_train_loss)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_inputs, batch_targets, _, val_input_masks, _, _ in val_dataloader:\n",
        "            batch_inputs, batch_targets = batch_inputs.to(device), batch_targets.to(device)\n",
        "            val_input_masks = val_input_masks.to(device)\n",
        "\n",
        "            outputs = model(batch_inputs, attention_mask=val_input_masks)\n",
        "            loss = loss_fn(outputs.logits.view(-1, outputs.logits.size(-1)), batch_targets.view(-1))\n",
        "            total_val_loss += loss.item()\n",
        "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
        "    print(f\"Epoch {epoch}, Training Loss: {avg_train_loss}, Validation Loss: {avg_val_loss}\")\n",
        "\n",
        "    # Early Stopping\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        no_improve = 0\n",
        "    else:\n",
        "        no_improve += 1\n",
        "    if no_improve == patience:\n",
        "        print(\"Early stopping!\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "b7a65315",
      "metadata": {
        "id": "b7a65315"
      },
      "outputs": [],
      "source": [
        "def post_process(response_text):\n",
        "    response_text = re.sub(' +', ' ', response_text)\n",
        "    response_text = response_text.strip()\n",
        "\n",
        "    # Truncate if the response is too long\n",
        "    if len(response_text.split()) > 100:\n",
        "        response_text = ' '.join(response_text.split()[:100]) + \"...\"\n",
        "\n",
        "    # Punctuation\n",
        "    if not response_text[-1] in ['.', '!', '?']:\n",
        "        response_text += '.'\n",
        "\n",
        "    return response_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "4a527bbc",
      "metadata": {
        "id": "4a527bbc"
      },
      "outputs": [],
      "source": [
        "def get_response(input_text, model, tokenizer, num_beams=7, max_length=80, temperature=0.5, top_k=50, no_repeat_ngram_size=2):\n",
        "    model.eval()\n",
        "\n",
        "    encoded_input = tokenizer.encode(input_text, return_tensors='pt', truncation=True)\n",
        "    input_length = encoded_input.shape[1]\n",
        "\n",
        "    if input_length + 10 > max_length:\n",
        "        max_length = input_length + 10\n",
        "\n",
        "    input_tensor = encoded_input.to(device)\n",
        "    attention_mask = torch.ones(input_tensor.shape).to(device)\n",
        "\n",
        "    # Beam search\n",
        "    output_ids = model.generate(\n",
        "        input_tensor,\n",
        "        max_length=max_length,\n",
        "        num_beams=num_beams,\n",
        "        early_stopping=True,\n",
        "        attention_mask=attention_mask,\n",
        "        pad_token_id=tokenizer.eos_token_id,  # Set pad_token_id\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "        no_repeat_ngram_size=no_repeat_ngram_size\n",
        "    )\n",
        "\n",
        "\n",
        "    response = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    response = post_process(response)\n",
        "\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "db22ae23",
      "metadata": {
        "id": "db22ae23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b995e260-7dd6-4931-ad03-d97caef3fd7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello in from the other side.\n"
          ]
        }
      ],
      "source": [
        "input_query = \"Hello\"\n",
        "response = get_response(input_query, model, tokenizer)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# Ensure you have downloaded the NLTK data\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "# Reference responses (ground truth)\n",
        "reference_responses = [\n",
        "    \"women like that have a way of turning professionals into amateurs.\",\n",
        "    \"wade, that was not smarch. going out right after the man s father in law shot himself. let it go. call it a favor to me.\",\n",
        "    \"it is scurvy s. his date got convicted. where did you get the dress?\",\n",
        "    \"not bad.\",\n",
        "    \"yes, it is.\"\n",
        "]\n",
        "\n",
        "# Clean the reference responses using your TextPreprocessor\n",
        "preprocessor = TextPreprocessor(max_len=30)  # Use the same max_len value as in your model preprocessing\n",
        "reference_responses = [preprocessor.clean_text(response) for response in reference_responses]\n",
        "\n",
        "# Model-generated responses\n",
        "input_queries = [\n",
        "    \"hey. i m a professional.\",\n",
        "    \"he say anything about the summons i tried to give him? sonofabitch would not accept it.\",\n",
        "    \"how did you get a tux at the last minute?\",\n",
        "    \"how are you feeling, fernando?\",\n",
        "    \"too bad about frank, is not it?\"\n",
        "]\n",
        "\n",
        "model_responses = []\n",
        "\n",
        "for query in input_queries:\n",
        "    response = get_response(query, model, tokenizer)  # Replace 'model' and 'tokenizer' with your actual model and tokenizer\n",
        "    model_responses.append(response)\n",
        "\n",
        "# Clean the model-generated responses using your TextPreprocessor\n",
        "model_responses = [preprocessor.clean_text(response) for response in model_responses]\n",
        "\n",
        "# BLEU Calculation\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "bleu_scores = [sentence_bleu([ref.split()], model.split()) for ref, model in zip(reference_responses, model_responses)]\n",
        "average_bleu = sum(bleu_scores) / len(bleu_scores)\n",
        "\n",
        "# ROUGE Calculation\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "rouge1_scores = []\n",
        "rouge2_scores = []\n",
        "rougeL_scores = []\n",
        "\n",
        "for ref, model in zip(reference_responses, model_responses):\n",
        "    scores = scorer.score(ref, model)\n",
        "    rouge1_scores.append(scores['rouge1'].fmeasure)\n",
        "    rouge2_scores.append(scores['rouge2'].fmeasure)\n",
        "    rougeL_scores.append(scores['rougeL'].fmeasure)\n",
        "\n",
        "average_rouge1 = sum(rouge1_scores) / len(rouge1_scores)\n",
        "average_rouge2 = sum(rouge2_scores) / len(rouge2_scores)\n",
        "average_rougeL = sum(rougeL_scores) / len(rougeL_scores)\n",
        "\n",
        "print(\"Average BLEU Score:\", average_bleu)\n",
        "print(\"Average ROUGE-1 Score:\", average_rouge1)\n",
        "print(\"Average ROUGE-2 Score:\", average_rouge2)\n",
        "print(\"Average ROUGE-L Score:\", average_rougeL)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OiY2sBRZWrS",
        "outputId": "3b5d4fa9-5a0b-4f79-e39b-899ed7b9f1d1"
      },
      "id": "5OiY2sBRZWrS",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average BLEU Score: 5.3160331857435324e-79\n",
            "Average ROUGE-1 Score: 0.22476190476190477\n",
            "Average ROUGE-2 Score: 0.03636363636363637\n",
            "Average ROUGE-L Score: 0.1657142857142857\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}