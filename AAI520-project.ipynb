{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5b4c0a0",
   "metadata": {
    "executionInfo": {
     "elapsed": 171,
     "status": "ok",
     "timestamp": 1696283722724,
     "user": {
      "displayName": "Alec Anderson",
      "userId": "10628656018086214733"
     },
     "user_tz": 420
    },
    "id": "e5b4c0a0"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "# Metrics\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "# Text box\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2GdIDHUhWmeX",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1696283249751,
     "user": {
      "displayName": "Alec Anderson",
      "userId": "10628656018086214733"
     },
     "user_tz": 420
    },
    "id": "2GdIDHUhWmeX"
   },
   "outputs": [],
   "source": [
    "# Read-in movie lines and conversations datasets\n",
    "movie_lines_path = 'movie_lines.txt'\n",
    "movie_conversations_path = 'movie_conversations.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bac7002",
   "metadata": {
    "executionInfo": {
     "elapsed": 267,
     "status": "ok",
     "timestamp": 1696283250014,
     "user": {
      "displayName": "Alec Anderson",
      "userId": "10628656018086214733"
     },
     "user_tz": 420
    },
    "id": "0bac7002"
   },
   "outputs": [],
   "source": [
    "lines = open(movie_lines_path, encoding='utf-8', errors='ignore').read().split('\\n')\n",
    "convers = open(movie_conversations_path, encoding='utf-8', errors='ignore').read().split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5efb0e",
   "metadata": {},
   "source": [
    "**Text Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ab8de4d",
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1696283250014,
     "user": {
      "displayName": "Alec Anderson",
      "userId": "10628656018086214733"
     },
     "user_tz": 420
    },
    "id": "9ab8de4d"
   },
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    def __init__(self, max_len=13):\n",
    "        self.max_len = max_len\n",
    "        self._compile_regex()\n",
    "\n",
    "    def _compile_regex(self):\n",
    "        self.regex_patterns = {\n",
    "            r\"i'm\": \"i am\",\n",
    "            r\"he's\": \"he is\",\n",
    "            r\"she's\": \"she is\",\n",
    "            r\"that's\": \"that is\",\n",
    "            r\"what's\": \"what is\",\n",
    "            r\"where's\": \"where is\",\n",
    "            r\"\\'ll\": \" will\",\n",
    "            r\"\\'ve\": \" have\",\n",
    "            r\"\\'re\": \" are\",\n",
    "            r\"\\'d\": \" would\",\n",
    "            r\"won't\": \"will not\",\n",
    "            r\"can't\": \"cannot\",\n",
    "            r\"[^\\w\\s]\": \"\",\n",
    "        }\n",
    "        self.compiled_patterns = {re.compile(pattern): repl for pattern, repl in self.regex_patterns.items()}\n",
    "\n",
    "    def clean_text(self, txt):\n",
    "        txt = txt.lower()\n",
    "        for pattern, repl in self.compiled_patterns.items():\n",
    "            txt = pattern.sub(repl, txt)\n",
    "        return txt\n",
    "\n",
    "    def preprocess_data(self, convers, lines):\n",
    "        exchange = [conver.split(' +++$+++ ')[-1][1:-1].replace(\"'\", \" \").replace(\",\", \"\").split() for conver in convers]\n",
    "        diag = {line.split(' +++$+++ ')[0]: line.split(' +++$+++ ')[-1] for line in lines}\n",
    "        questions, answers = self._extract_questions_answers(exchange, diag)\n",
    "        return questions, answers\n",
    "\n",
    "    def _extract_questions_answers(self, exchange, diag):\n",
    "        questions, answers = [], []\n",
    "        for conver in exchange:\n",
    "            for i in range(len(conver) - 1):\n",
    "                questions.append(diag.get(conver[i], ''))\n",
    "                answers.append(diag.get(conver[i + 1], ''))\n",
    "        sorted_ques = [q for q in questions if len(q.split()) < self.max_len]\n",
    "        sorted_ans = [a for q, a in zip(questions, answers) if len(q.split()) < self.max_len]\n",
    "        return sorted_ques, sorted_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff0e1d9a",
   "metadata": {
    "executionInfo": {
     "elapsed": 988,
     "status": "ok",
     "timestamp": 1696283563711,
     "user": {
      "displayName": "Alec Anderson",
      "userId": "10628656018086214733"
     },
     "user_tz": 420
    },
    "id": "ff0e1d9a"
   },
   "outputs": [],
   "source": [
    "# Apply preprocessing and cleaning\n",
    "max_len = 13\n",
    "preprocessor = TextPreprocessor(max_len=max_len)\n",
    "sorted_ques, sorted_ans = preprocessor.preprocess_data(convers, lines)\n",
    "clean_ques = [preprocessor.clean_text(q) for q in sorted_ques]\n",
    "clean_ans = [preprocessor.clean_text(a) for a in sorted_ans]\n",
    "\n",
    "# Trimming answers and lists\n",
    "clean_ans = [' '.join(ans.split()[:11]) for ans in clean_ans]\n",
    "clean_ans = clean_ans[:30000]\n",
    "clean_ques = clean_ques[:30000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79e1a023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Well, I thought we'd start with pronunciation, if that's okay with you.\", 'Not the hacking and gagging and spitting part.  Please.', \"You're asking me out.  That's so cute. What's your name again?\", 'Cameron.', 'Why?']\n",
      "['Not the hacking and gagging and spitting part.  Please.', \"Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\", 'Forget it.', \"The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\", 'Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.']\n"
     ]
    }
   ],
   "source": [
    "# Before cleaning\n",
    "print(sorted_ques[:5])\n",
    "print(sorted_ans[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7eb9bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['well i thought we would start with pronunciation if that is okay with you', 'not the hacking and gagging and spitting part  please', 'you are asking me out  that is so cute what is your name again', 'cameron', 'why']\n",
      "['not the hacking and gagging and spitting part please', 'okay then how bout we try out some french cuisine saturday', 'forget it', 'the thing is cameron i am at the mercy of a', 'unsolved mystery she used to be really popular when she started']\n"
     ]
    }
   ],
   "source": [
    "# After cleaning\n",
    "print(clean_ques[:5])\n",
    "print(clean_ans[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "Y_JQIi1mYH2r",
   "metadata": {
    "executionInfo": {
     "elapsed": 202,
     "status": "ok",
     "timestamp": 1696283566047,
     "user": {
      "displayName": "Alec Anderson",
      "userId": "10628656018086214733"
     },
     "user_tz": 420
    },
    "id": "Y_JQIi1mYH2r"
   },
   "outputs": [],
   "source": [
    "# Counting word occurrences and creating vocab based on threshold\n",
    "word2count = {}\n",
    "for line in clean_ques + clean_ans:\n",
    "    for word in line.split():\n",
    "        word2count[word] = word2count.get(word, 0) + 1\n",
    "\n",
    "# Creating the vocabulary based on threshold\n",
    "thresh = 5\n",
    "vocab = {word: num for num, (word, count) in enumerate(word2count.items()) if count >= thresh}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "qlT6Ec8kYH46",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 432,
     "status": "ok",
     "timestamp": 1696283568465,
     "user": {
      "displayName": "Alec Anderson",
      "userId": "10628656018086214733"
     },
     "user_tz": 420
    },
    "id": "qlT6Ec8kYH46",
    "outputId": "ec9cd3f3-2ecf-42d9-ec7f-6ec3fcd3a493"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[0, 1, 2, 3, 4, 5, 6, 4088, 8, 9, 10, 11, 6, 12],\n",
       "  [13, 14, 4088, 16, 4088, 16, 4088, 19, 20],\n",
       "  [12, 21, 22, 23, 24, 9, 10, 25, 26, 27, 10, 28, 29, 30],\n",
       "  [31],\n",
       "  [32]],\n",
       " [[4089, 13, 14, 4088, 16, 4088, 16, 4088, 19, 20, 4087],\n",
       "  [4089, 11, 93, 56, 1129, 3, 738, 24, 361, 283, 4088, 141, 4087],\n",
       "  [4089, 166, 159, 4087],\n",
       "  [4089, 14, 140, 10, 31, 1, 103, 133, 14, 3364, 81, 38, 4087],\n",
       "  [4089, 4088, 8415, 79, 271, 75, 125, 430, 945, 365, 79, 1107, 4087]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding special tokens to the vocabulary\n",
    "tokens = ['<PAD>', '<EOS>', '<OUT>', '<SOS>']\n",
    "for token in tokens:\n",
    "    vocab[token] = len(vocab)\n",
    "\n",
    "# Creating the inverse vocabulary\n",
    "inv_vocab = {num: word for word, num in vocab.items()}\n",
    "\n",
    "# Encoding the questions and answers\n",
    "encoder_inp = [[vocab.get(word, vocab['<OUT>']) for word in line.split()] for line in clean_ques]\n",
    "decoder_inp = [[vocab.get(word, vocab['<OUT>']) for word in ('<SOS> ' + line + ' <EOS>').split()] for line in clean_ans]\n",
    "\n",
    "# Sample output for verification\n",
    "encoder_inp[:5], decoder_inp[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31709367",
   "metadata": {
    "executionInfo": {
     "elapsed": 1800,
     "status": "ok",
     "timestamp": 1696283934857,
     "user": {
      "displayName": "Alec Anderson",
      "userId": "10628656018086214733"
     },
     "user_tz": 420
    },
    "id": "31709367"
   },
   "outputs": [],
   "source": [
    "# Convert lists to PyTorch tensors\n",
    "encoder_inp_tensors = [torch.LongTensor(seq) for seq in encoder_inp]\n",
    "decoder_inp_tensors = [torch.LongTensor(seq) for seq in decoder_inp]\n",
    "\n",
    "# Padding\n",
    "max_seq_len = max(max(len(seq) for seq in encoder_inp_tensors), max(len(seq) for seq in decoder_inp_tensors))\n",
    "encoder_inp_tensors_padded = [torch.cat([seq, torch.LongTensor([vocab['<PAD>']] * (max_seq_len - len(seq)))]) for seq in encoder_inp_tensors]\n",
    "decoder_inp_tensors_padded = [torch.cat([seq, torch.LongTensor([vocab['<PAD>']] * (max_seq_len - len(seq)))]) for seq in decoder_inp_tensors]\n",
    "\n",
    "# Convert lists of tensors to 2D tensors\n",
    "encoder_inp_padded = torch.stack(encoder_inp_tensors_padded, dim=0)\n",
    "decoder_inp_padded = torch.stack(decoder_inp_tensors_padded, dim=0)\n",
    "decoder_final_output = decoder_inp_padded[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52d6498a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30000, 15])\n",
      "torch.Size([30000, 15])\n",
      "torch.Size([30000, 14])\n"
     ]
    }
   ],
   "source": [
    "print(encoder_inp_padded.shape)\n",
    "print(decoder_inp_padded.shape)\n",
    "print(decoder_final_output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c79f346-5c07-47c8-a751-fb099b981bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30000, 14])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_final_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "Qa_ZkS2rYxSl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 188,
     "status": "ok",
     "timestamp": 1696283704542,
     "user": {
      "displayName": "Alec Anderson",
      "userId": "10628656018086214733"
     },
     "user_tz": 420
    },
    "id": "Qa_ZkS2rYxSl",
    "outputId": "0f2ac5fd-1ecc-4b03-8830-13785de82653"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([30000, 15]), torch.Size([30000, 15]), torch.Size([30000, 14]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify\n",
    "encoder_inp_padded.shape, decoder_inp_padded.shape, decoder_final_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ba13e7f-90b6-43d3-a713-ba3f42701dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  93,    9,   10,   94,   12,   95,   75,   96, 4086, 4086, 4086, 4086,\n",
       "        4086, 4086, 4086])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_inp_padded[22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12b0c0a8-f7ae-4fcd-a1fc-0e30af282754",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\beand\\AppData\\Local\\Temp\\ipykernel_14364\\1393693452.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_batch = torch.tensor(encoder_inp_padded[2].to(dtype=torch.long))\n",
      "C:\\Users\\beand\\AppData\\Local\\Temp\\ipykernel_14364\\1393693452.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_batch2 = torch.tensor(encoder_inp_padded[1].to(dtype=torch.long))\n",
      "C:\\Users\\beand\\AppData\\Local\\Temp\\ipykernel_14364\\1393693452.py:3: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3575.)\n",
      "  batch_tester = torch.stack([test_batch.T,test_batch2])\n"
     ]
    }
   ],
   "source": [
    "test_batch = torch.tensor(encoder_inp_padded[2].to(dtype=torch.long))\n",
    "test_batch2 = torch.tensor(encoder_inp_padded[1].to(dtype=torch.long))\n",
    "batch_tester = torch.stack([test_batch.T,test_batch2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f88c371",
   "metadata": {},
   "source": [
    "**Encoder, Decoder and Seq2Seq**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c761e8ce",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1696283257085,
     "user": {
      "displayName": "Alec Anderson",
      "userId": "10628656018086214733"
     },
     "user_tz": 420
    },
    "id": "c761e8ce"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, bidirectional=True)\n",
    "\n",
    "    def forward(self, input_seq, input_lengths, hidden=None):\n",
    "        embedded = self.embedding(input_seq)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths.cpu(), enforce_sorted=False)\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, :, self.hidden_size:]\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9e69c5d-4a9a-4f41-b0c9-b2b2b05563a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "            \n",
    "    def general_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(encoder_output)\n",
    "        return torch.sum(hidden * energy, dim=2)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        if self.method == 'general':\n",
    "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
    "        \n",
    "        attn_energies = attn_energies.t()\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44595cf1-aba9-4491-8de6-ef04eb984ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self , attn_model , embedding , hidden_size , output_size , n_layers = 1 , dropout = 0.1):\n",
    "        super(LuongAttnDecoderRNN , self).__init__()\n",
    "        self.attn_model = attn_model \n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size =output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        self.embedding = embedding \n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size , hidden_size , n_layers , dropout = 0 , )\n",
    "        self.concat = nn.Linear(hidden_size*2 , hidden_size)\n",
    "        self.out = nn.Linear(hidden_size , output_size)\n",
    "        self.attn = Attn(attn_model , hidden_size)\n",
    "    def forward(self , input_step , last_hidden , encoder_outputs):\n",
    "        embedded = self.embedding(input_step)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        rnn_output , hidden = self.gru(embedded , last_hidden)\n",
    "        attn_weights = self.attn(rnn_output , encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0,1)) \n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        context = context.squeeze(1)\n",
    "        concat_input = torch.cat((rnn_output , context) , 1)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "        output = self.out(concat_output)\n",
    "        output = F.softmax(output , dim = 1)\n",
    "        return output , hidden  \n",
    "\n",
    "\n",
    "input_size = len(vocab)\n",
    "src_vocab_size = len(vocab)\n",
    "trg_vocab_size = len(vocab)\n",
    "hidden_size = 200 \n",
    "\n",
    "encoder = Encoder(input_size, hidden_size)\n",
    "\n",
    "test_input = encoder_inp_padded[1].unsqueeze(1)\n",
    "test_input_lengths = torch.tensor([len(test_input)]) \n",
    "\n",
    "# Double checking sequence length:\n",
    "assert test_input.shape[0] >= torch.max(test_input_lengths), \"Declared sequence length exceeds actual length\"\n",
    "\n",
    "# Encoder\n",
    "output, hidden = encoder(test_input, test_input_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c181fc60-998d-492a-acc6-ffdc1f4e0e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bcd2ed7d-e1e9-4dc2-af28-bf4f59598298",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = Attn(\"general\", 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b964af52-3bf1-47d6-8413-06cf1a68e3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(len(vocab), hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2adb3e0b-e632-4c13-8ac2-80da8a3da9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Decoder = LuongAttnDecoderRNN(attn, embedding,  hidden_size, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c7fdefdf-9a3e-422b-8ba6-11228ffb9384",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = Encoder(1, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93f688a9-cc95-4579-859b-9ba79a9d9b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = len(vocab)\n",
    "src_vocab_size = len(vocab)\n",
    "trg_vocab_size = len(vocab)\n",
    "hidden_size = 200 \n",
    "\n",
    "encoder = Encoder(input_size, hidden_size)\n",
    "\n",
    "test_input = encoder_inp_padded[1].unsqueeze(1)\n",
    "test_input_lengths = torch.tensor([len(test_input)]) \n",
    "\n",
    "# Double checking sequence length:\n",
    "assert test_input.shape[0] >= torch.max(test_input_lengths), \"Declared sequence length exceeds actual length\"\n",
    "\n",
    "# Encoder\n",
    "output, hidden = encoder(batch_tester.T, torch.tensor([15,15]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5098a3d-ee17-4817-a184-972e8e0499f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_hidden = hidden[:Decoder.n_layers]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1734c28f-1c38-4c97-87b6-408218924828",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 200])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_hidden.squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "74c5615a-1465-4ab9-8775-1b6baf1c76e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 200])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d1df0d48-7430-465e-b50d-7659f918856c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4089"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab['<SOS>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ab47c81c-d9e9-4c2c-8be3-6aa47ebce79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = torch.LongTensor([[vocab['<SOS>'] for _ in range(200)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8f79ca70-6420-40dc-a57f-6305a8761b82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3228, -0.1422, -0.5230,  0.4568,  0.3626,  0.2242, -0.1720,\n",
       "          -0.0801,  0.1019, -0.1879, -0.4286,  0.3959,  0.2633,  0.6614,\n",
       "           0.1586,  0.2969,  0.1604, -0.0280,  0.1978, -0.2136,  0.1204,\n",
       "           0.0258,  0.2063, -0.1129, -0.1605, -0.0709, -0.1737,  0.2445,\n",
       "           0.6190, -0.4989,  0.3018,  0.2556,  0.3671, -0.2218, -0.3314,\n",
       "           0.1250, -0.3679, -0.2191, -0.1800,  0.0756, -0.1976,  0.2722,\n",
       "          -0.3354, -0.4069,  0.2706, -0.0576,  0.3223,  0.0678, -0.0725,\n",
       "          -0.0770,  0.3896,  0.0561,  0.2765,  0.2332, -0.1978, -0.1619,\n",
       "          -0.1649,  0.3653, -0.6702, -0.2230,  0.5320, -0.2824,  0.0691,\n",
       "          -0.3573,  0.3368, -0.1237,  0.1509,  0.3799, -0.3058,  0.2999,\n",
       "           0.3328,  0.2259, -0.1222, -0.1998,  0.0750,  0.1757, -0.1996,\n",
       "          -0.0266,  0.2411, -0.2798,  0.2200,  0.0684, -0.2340,  0.3991,\n",
       "           0.1005, -0.3144, -0.1802,  0.4687, -0.3278, -0.4043, -0.5034,\n",
       "           0.1587,  0.6539, -0.0572,  0.0345,  0.4362, -0.5572, -0.1753,\n",
       "          -0.0332,  0.2166,  0.0455, -0.0682, -0.3523, -0.1306, -0.1913,\n",
       "          -0.1039,  0.0998,  0.2844, -0.0346,  0.1816,  0.5305, -0.3424,\n",
       "           0.5263,  0.1651,  0.3734,  0.3842,  0.3559, -0.2750, -0.2025,\n",
       "           0.0891,  0.1827, -0.2069, -0.2743, -0.2796,  0.2560,  0.1799,\n",
       "          -0.2131,  0.0852, -0.0142,  0.0085,  0.1021,  0.0401,  0.1344,\n",
       "           0.3795, -0.0319, -0.0931,  0.0215,  0.0516,  0.2169, -0.5183,\n",
       "          -0.0969, -0.2207, -0.1232,  0.1825, -0.2042, -0.0535,  0.1991,\n",
       "          -0.4195, -0.0424, -0.2975,  0.1222, -0.3417,  0.0502, -0.1380,\n",
       "           0.2261,  0.1967,  0.3799,  0.3611, -0.0845, -0.1666,  0.1817,\n",
       "           0.0971,  0.0772, -0.2514, -0.2588,  0.3572,  0.1838, -0.2402,\n",
       "          -0.1218, -0.0200,  0.1952, -0.1878, -0.3027, -0.2340, -0.0115,\n",
       "           0.3549,  0.4403, -0.1910, -0.2361, -0.4079, -0.4947, -0.1654,\n",
       "           0.0360,  0.4524, -0.3728, -0.0208, -0.0116, -0.0403,  0.0135,\n",
       "          -0.2318,  0.1126,  0.2924,  0.3023,  0.5167, -0.0593, -0.3703,\n",
       "           0.1241, -0.1830, -0.0441,  0.5176],\n",
       "         [-0.5215, -0.0192, -0.7148,  0.6638, -0.0554,  0.2465,  0.5122,\n",
       "          -0.5815, -0.0748,  0.0282, -0.3471, -0.0238,  0.7371,  0.8568,\n",
       "          -0.2575,  0.4435,  0.4772,  0.4378,  0.3720, -0.1525,  0.2906,\n",
       "          -0.1689,  0.5989, -0.0844,  0.1119,  0.4362, -0.5591,  0.6645,\n",
       "           0.7265, -0.2987,  0.3102,  0.5099,  0.6186, -0.5702, -0.7981,\n",
       "           0.3539, -0.7130,  0.0605, -0.0414,  0.4455, -0.4537,  0.4439,\n",
       "          -0.1551, -0.4585,  0.3445, -0.2203,  0.3506,  0.3989, -0.1215,\n",
       "          -0.3543,  0.1831, -0.0045, -0.3017,  0.6100, -0.6341, -0.3356,\n",
       "          -0.3638,  0.6870, -0.8195, -0.3135,  0.3785, -0.5779, -0.0925,\n",
       "          -0.7036,  0.6020, -0.4019, -0.1131,  0.1448, -0.3170,  0.0013,\n",
       "          -0.1165,  0.6848, -0.1562, -0.7796,  0.2805,  0.4798, -0.6592,\n",
       "          -0.5163,  0.7352, -0.7353,  0.7939,  0.6390, -0.5456,  0.4586,\n",
       "           0.3243,  0.3471,  0.2160,  0.6665, -0.4028, -0.5288, -0.2647,\n",
       "           0.3585,  0.7448,  0.0767, -0.3314,  0.8623, -0.7104,  0.0762,\n",
       "           0.3354,  0.5698,  0.1113,  0.0582, -0.0525, -0.3860, -0.3327,\n",
       "          -0.7159,  0.2186,  0.0757,  0.0052,  0.1696,  0.7938, -0.6535,\n",
       "           0.8175,  0.0848,  0.7218, -0.5416,  0.7400, -0.6099,  0.0416,\n",
       "          -0.0266,  0.2093, -0.2634, -0.3044, -0.8952, -0.0965,  0.4336,\n",
       "          -0.4218, -0.6375, -0.1076,  0.3657,  0.3088, -0.0098, -0.0967,\n",
       "           0.5413,  0.4275,  0.2415, -0.0028,  0.0257,  0.4111, -0.7252,\n",
       "           0.0781, -0.3577, -0.5945,  0.7781,  0.0928, -0.2415,  0.4978,\n",
       "          -0.1586, -0.2137,  0.1238, -0.1541, -0.4843, -0.5084, -0.0571,\n",
       "           0.6420,  0.3767,  0.7875,  0.7991, -0.0679,  0.1615,  0.6521,\n",
       "           0.3155,  0.3093, -0.7515, -0.4178,  0.5139,  0.1226, -0.8557,\n",
       "           0.0980, -0.4028,  0.4119, -0.3417, -0.5900, -0.2381,  0.0669,\n",
       "           0.3609,  0.5723, -0.5194, -0.3907, -0.2412, -0.7143, -0.4321,\n",
       "           0.7543,  0.4828, -0.4779,  0.5486, -0.4978,  0.0569, -0.4768,\n",
       "          -0.4452, -0.2245,  0.5040,  0.6628,  0.6437,  0.2621, -0.5204,\n",
       "           0.1831,  0.1124, -0.2076,  0.9208]]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "154d1a74-66cf-431a-80ec-20a8c1680587",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected hidden size (1, 200, 200), got [1, 2, 200]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m decoder_output, decoder_hidden \u001b[38;5;241m=\u001b[39m \u001b[43mDecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_hidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\Ana\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[19], line 18\u001b[0m, in \u001b[0;36mLuongAttnDecoderRNN.forward\u001b[1;34m(self, input_step, last_hidden, encoder_outputs)\u001b[0m\n\u001b[0;32m     16\u001b[0m embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(input_step)\n\u001b[0;32m     17\u001b[0m embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_dropout(embedded)\n\u001b[1;32m---> 18\u001b[0m rnn_output , hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgru\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedded\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_hidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(rnn_output , encoder_outputs)\n\u001b[0;32m     20\u001b[0m context \u001b[38;5;241m=\u001b[39m attn_weights\u001b[38;5;241m.\u001b[39mbmm(encoder_outputs\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m)) \n",
      "File \u001b[1;32mE:\\Ana\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mE:\\Ana\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:996\u001b[0m, in \u001b[0;36mGRU.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    991\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    992\u001b[0m     \u001b[38;5;66;03m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[0;32m    993\u001b[0m     \u001b[38;5;66;03m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[0;32m    994\u001b[0m     hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m--> 996\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_forward_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    997\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    998\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mgru(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[0;32m    999\u001b[0m                      \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n",
      "File \u001b[1;32mE:\\Ana\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:256\u001b[0m, in \u001b[0;36mRNNBase.check_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_input(\u001b[38;5;28minput\u001b[39m, batch_sizes)\n\u001b[0;32m    254\u001b[0m expected_hidden_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_hidden_size(\u001b[38;5;28minput\u001b[39m, batch_sizes)\n\u001b[1;32m--> 256\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_hidden_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_hidden_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\Ana\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:239\u001b[0m, in \u001b[0;36mRNNBase.check_hidden_size\u001b[1;34m(self, hx, expected_hidden_size, msg)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_hidden_size\u001b[39m(\u001b[38;5;28mself\u001b[39m, hx: Tensor, expected_hidden_size: Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m],\n\u001b[0;32m    237\u001b[0m                       msg: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hx\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m expected_hidden_size:\n\u001b[1;32m--> 239\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mformat(expected_hidden_size, \u001b[38;5;28mlist\u001b[39m(hx\u001b[38;5;241m.\u001b[39msize())))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected hidden size (1, 200, 200), got [1, 2, 200]"
     ]
    }
   ],
   "source": [
    "decoder_output, decoder_hidden = Decoder(decoder_input, decoder_hidden, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "45de4d10-2e1f-4364-81fb-6abb7ce7471e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 15])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_tester.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "cca6de39-261a-4acc-a14b-c719e9be2276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 200])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "427abc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerSeq2Seq(nn.Module):\n",
    "    def __init__(self, src_vocab_size, trg_vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers):\n",
    "        super(TransformerSeq2Seq, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model, nhead),\n",
    "            num_layers=num_encoder_layers\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.TransformerDecoder(\n",
    "            nn.TransformerDecoderLayer(d_model, nhead),\n",
    "            num_layers=num_decoder_layers\n",
    "        )\n",
    "\n",
    "        # Embeddings\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.trg_embedding = nn.Embedding(trg_vocab_size, d_model)\n",
    "\n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(d_model, trg_vocab_size)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        # Embed source and target sequences\n",
    "        src_embedded = self.src_embedding(src)\n",
    "        trg_embedded = self.trg_embedding(trg)\n",
    "\n",
    "        # Encode source sequence\n",
    "        encoder_output = self.encoder(src_embedded)\n",
    "\n",
    "        # Decode target sequence with encoder output\n",
    "        decoder_output = self.decoder(trg_embedded, encoder_output)\n",
    "\n",
    "        # Final prediction\n",
    "        output = self.output_layer(decoder_output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dda9a72e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "executionInfo": {
     "elapsed": 1377,
     "status": "error",
     "timestamp": 1696283258460,
     "user": {
      "displayName": "Alec Anderson",
      "userId": "10628656018086214733"
     },
     "user_tz": 420
    },
    "id": "dda9a72e",
    "outputId": "8968ae54-cb82-48b7-ee62-9a28277e6602"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Loss: 6.1925\n",
      "Epoch [1/2], Loss: 5.1615\n",
      "Epoch [1/2], Loss: 5.8120\n",
      "Epoch [1/2], Loss: 4.7357\n",
      "Epoch [1/2], Loss: 5.3043\n",
      "Epoch [1/2], Loss: 4.6702\n",
      "Epoch [1/2], Loss: 5.9605\n",
      "Epoch [1/2], Loss: 5.8685\n",
      "Epoch [1/2], Loss: 5.3909\n",
      "Epoch [1/2], Loss: 5.7470\n",
      "Epoch [1/2], Loss: 5.3402\n",
      "Epoch [1/2], Loss: 5.0852\n",
      "Epoch [1/2], Loss: 5.5482\n",
      "Epoch [1/2], Loss: 5.4563\n",
      "Epoch [1/2], Loss: 5.8816\n",
      "Epoch [1/2], Loss: 5.0328\n",
      "Epoch [1/2], Loss: 4.5945\n",
      "Epoch [1/2], Loss: 6.1544\n",
      "Epoch [1/2], Loss: 5.7120\n",
      "Epoch [1/2], Loss: 4.5364\n",
      "Epoch [1/2], Loss: 5.5490\n",
      "Epoch [1/2], Loss: 5.0505\n",
      "Epoch [1/2], Loss: 5.0775\n",
      "Epoch [1/2], Loss: 5.9997\n",
      "Epoch [1/2], Loss: 5.6600\n",
      "Epoch [1/2], Loss: 4.6193\n",
      "Epoch [2/2], Loss: 5.2666\n",
      "Epoch [2/2], Loss: 4.7714\n",
      "Epoch [2/2], Loss: 5.0579\n",
      "Epoch [2/2], Loss: 4.8656\n",
      "Epoch [2/2], Loss: 4.3671\n",
      "Epoch [2/2], Loss: 5.0789\n",
      "Epoch [2/2], Loss: 5.1490\n",
      "Epoch [2/2], Loss: 4.7253\n",
      "Epoch [2/2], Loss: 5.2326\n",
      "Epoch [2/2], Loss: 5.7042\n",
      "Epoch [2/2], Loss: 4.2576\n",
      "Epoch [2/2], Loss: 6.0411\n",
      "Epoch [2/2], Loss: 4.6721\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 2\n",
    "LR = 0.001\n",
    "\n",
    "# Defining parameters\n",
    "d_model = 512\n",
    "nhead = 8\n",
    "num_encoder_layers = 6\n",
    "num_decoder_layers = 6\n",
    "\n",
    "model = TransformerSeq2Seq(src_vocab_size, trg_vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab['<PAD>'])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    data_loader = DataLoader(\n",
    "        list(zip(encoder_inp_padded, decoder_inp_padded, decoder_final_output)),\n",
    "        batch_size=3,\n",
    "        shuffle=True\n",
    "    )\n",
    "    for i, (encoder_input, decoder_input, decoder_output) in enumerate(data_loader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if (encoder_input >= src_vocab_size).any() or (decoder_input >= trg_vocab_size).any():\n",
    "            continue\n",
    "\n",
    "        outputs = model(encoder_input, decoder_input)\n",
    "        \n",
    "        loss = criterion(outputs[:, 1:].contiguous().view(-1, outputs.shape[-1]), decoder_output.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch [{epoch + 1}/{EPOCHS}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c543ef5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'transformer_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b89430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:8080\n",
      " * Running on http://192.168.0.205:8080\n",
      "Press CTRL+C to quit\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import torch\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load the saved model\n",
    "model = TransformerSeq2Seq(src_vocab_size, trg_vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers)\n",
    "model.load_state_dict(torch.load('transformer_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    input_data = request.json  # Assuming you send data in JSON format\n",
    "    # Preprocess input_data if needed\n",
    "    # Perform inference using the model\n",
    "    # Return the model's predictions\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=8080)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfc0e67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
