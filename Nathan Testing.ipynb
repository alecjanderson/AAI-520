{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42e7df0c-e239-453a-aef8-c8ca6d777201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "# Metrics\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "# Text box\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496c47d7-99b7-48da-a548-e27085190b5c",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8635e07-a399-4732-9326-87bc2066ce61",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_lines_path = 'movie_lines.txt'\n",
    "movie_conversations_path = 'movie_conversations.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03e7e220-0946-4887-b3c1-7587e07eb911",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(movie_lines_path, encoding='iso-8859-1', errors='ignore') as my_file:\n",
    "    all_lines = {}\n",
    "    for line in my_file:\n",
    "        split = line.split(' +++$+++ ')\n",
    "        linemp = {}\n",
    "        fields = [\"lineID\", \"characterID\", \"movieID\", \"character\", \"text\"]\n",
    "        count = 0\n",
    "        for field in (fields):\n",
    "                linemp[field] = split[count]\n",
    "                count +=1\n",
    "        all_lines[linemp['lineID']] = linemp        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11228243-0624-4396-a869-dbeb6903ae1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(movie_conversations_path, encoding='iso-8859-1', errors='ignore') as my_file:\n",
    "    conv = []\n",
    "    for line in my_file:\n",
    "        split = line.split(' +++$+++ ')\n",
    "        obj = {}\n",
    "        fields = [\"character1ID\", \"character2ID\", \"movieID\", \"utteranceIDs\"]\n",
    "        count = 0 \n",
    "        for field in fields:\n",
    "            obj[field] = split[count]\n",
    "            count +=1\n",
    "        ID = re.compile('L[0-9]+').findall(obj['utteranceIDs'])\n",
    "        lines = []\n",
    "        \n",
    "        for id_ in ID:\n",
    "            lines.append(all_lines[id_])\n",
    "        obj['line'] = lines\n",
    "        conv.append(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9128b1c-a2d3-471c-9170-9c26cc9e471e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lineID': 'L985',\n",
       " 'characterID': 'u0',\n",
       " 'movieID': 'm0',\n",
       " 'character': 'BIANCA',\n",
       " 'text': 'I hope so.\\n'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_lines[\"L985\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe44ff11-e6cf-4031-9d9d-e11c59ad3a6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'character1ID': 'u0',\n",
       " 'character2ID': 'u2',\n",
       " 'movieID': 'm0',\n",
       " 'utteranceIDs': \"['L367', 'L368']\\n\",\n",
       " 'line': [{'lineID': 'L367',\n",
       "   'characterID': 'u2',\n",
       "   'movieID': 'm0',\n",
       "   'character': 'CAMERON',\n",
       "   'text': 'How do you get your hair to look like that?\\n'},\n",
       "  {'lineID': 'L368',\n",
       "   'characterID': 'u0',\n",
       "   'movieID': 'm0',\n",
       "   'character': 'BIANCA',\n",
       "   'text': \"Eber's Deep Conditioner every two days. And I never, ever use a blowdryer without the diffuser attachment.\\n\"}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e77a02e-cf5c-4e7a-82ef-79a35f6774c0",
   "metadata": {},
   "source": [
    "# Matching Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdd12473-b5da-4467-821f-47d072334dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = []\n",
    "for convrtsation in conv:\n",
    "        for i in range(len(convrtsation['line'])):\n",
    "            try:\n",
    "                question = convrtsation['line'][i]['text'].strip()\n",
    "                answer = convrtsation['line'][i+1]['text'].strip()\n",
    "            except:\n",
    "                pass\n",
    "            if(question and answer):\n",
    "                pairs.append([question, answer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3552951-7952-46ca-b2c0-ba77810a05aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "304309"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a04a318c-c4e0-4c34-96cf-e0899fa7486f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.', \"Well, I thought we'd start with pronunciation, if that's okay with you.\"]\n",
      "[\"Well, I thought we'd start with pronunciation, if that's okay with you.\", 'Not the hacking and gagging and spitting part.  Please.']\n",
      "['Not the hacking and gagging and spitting part.  Please.', \"Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\"]\n",
      "[\"Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\", \"Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\"]\n",
      "[\"You're asking me out.  That's so cute. What's your name again?\", 'Forget it.']\n",
      "['Forget it.', 'Forget it.']\n",
      "[\"No, no, it's my fault -- we didn't have a proper introduction ---\", 'Cameron.']\n",
      "['Cameron.', \"The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\"]\n",
      "[\"The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\", 'Seems like she could get a date easy enough...']\n",
      "['Seems like she could get a date easy enough...', 'Seems like she could get a date easy enough...']\n"
     ]
    }
   ],
   "source": [
    "for i in range (10):\n",
    "    print(pairs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9643b90a-c167-4dd1-89a3-3fd652c336e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self):\n",
    "        self.enum = {\"PAD_token\" : 0, \"SOS_token\" : 1, \"EOS_token\":2, \"UNK\":3}\n",
    "        self.count = {}\n",
    "        self.index = {}\n",
    "        self.wordcount = 4\n",
    "        self.min_freq = 10\n",
    "    def addSentence(self,sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            if word not in self.enum:\n",
    "                if(word in self.count.keys()):\n",
    "                    self.count[word] += 1\n",
    "                    if(self.count[word] >= self.min_freq):\n",
    "                        self.enum[word] = self.wordcount\n",
    "                        self.index[self.wordcount] = word\n",
    "                        self.wordcount += 1\n",
    "                else:\n",
    "                    self.count[word] = 1\n",
    "            else:\n",
    "                #print(\"Word already Added\")\n",
    "                self.count[word] += 1\n",
    "    def __len__(self):\n",
    "        return self.wordcount    \n",
    "                \n",
    "    ### This will be the class that handles the bag of words.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce72a784-ddc9-4c8e-9af9-f3ddd551693e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_token = 0\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "UNK = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a17b2f5-1baf-455e-a94e-81c91371af49",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = Vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e43e236b-1a29-4632-ad31-ee6958da64c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31e5db64-d0b7-4908-b9e7-2140f8a33210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(304309, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = np.array(pairs)\n",
    "shape.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35859fa4-0fc7-4ca6-9647-d4870023d8ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Not the hacking and gagging and spitting part.  Please.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29b1c1be-7c64-4a09-9e5a-154c74e35c43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ee80a932-7445-443e-bd7b-dcebc0e52437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PAD_token': 0, 'SOS_token': 1, 'EOS_token': 2, 'UNK': 3}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc.enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1900b858-db64-4911-a100-87ef6ac1e513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "899030fa-a68f-4838-ac08-4129570a3a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\beand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cfac3d31-cdb7-4e1f-8e85-d3aee04d03dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_String(string):\n",
    "    lower_string = string.lower()\n",
    "    no_number_string = re.sub(r'\\d+','',lower_string)\n",
    "    no_punc_string = re.sub(r'[^\\w\\s]','', no_number_string) \n",
    "    no_wspace_string = no_punc_string.strip()\n",
    "    \n",
    "    words = no_wspace_string.split()\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d1ae59f-6444-4907-8ec8-ec16a7c2d7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in pairs:\n",
    "    for j in i:\n",
    "        cleaned = clean_String(j)\n",
    "        voc.addSentence(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08815424-1c46-48b3-a683-075a0a50ebe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17148"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(voc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176600fb-6cba-44c8-a12a-60fa8187cd2c",
   "metadata": {},
   "source": [
    "# Make Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ee1fca1-fe0b-463f-8ab2-8fc1e8a02e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a8b76f26-08a9-4a1f-b04e-95988203b4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6e22a6e-ca49-40c4-8789-a47bd3e53b91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5337"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voc.enum[\"judgment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c734bf7f-3264-4dce-be7f-8c52e1af0dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15a83b6b-4465-405d-8a93-33fc427c35b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EnumerateSentance(voc , sentence):\n",
    "    sentence = clean_String(sentence)\n",
    "    words = sentence.split()\n",
    "    output = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            output.append(voc.enum[word])\n",
    "        except:\n",
    "            output.append(UNK)\n",
    "    output.append(EOS_token)\n",
    "    return output\n",
    "\n",
    "def out(string ,voc):\n",
    "    batches = []\n",
    "    for sen in string:\n",
    "            batches.append(EnumerateSentance(voc,sen))\n",
    "    max_l = 0\n",
    "    for sen in batches:\n",
    "        if(len(sen) > max_l):\n",
    "            max_l = len(sen)\n",
    "    padList = list(itertools.zip_longest(*batches , fillvalue = PAD_token))\n",
    "    mask = Mask(padList)\n",
    "    mask = torch.BoolTensor(mask)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar , mask , max_l\n",
    "\n",
    "\n",
    "def inps(string,voc):\n",
    "    indexes_batch = []\n",
    "    for i in string:\n",
    "        indexes_batch.append(EnumerateSentance(voc , i))\n",
    "    length = []\n",
    "    for i in indexes_batch:\n",
    "        length.append(len(i))\n",
    "    #print(length)\n",
    "    padList = list(itertools.zip_longest(*indexes_batch , fillvalue = PAD_token))\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar , length\n",
    "\n",
    "def Mask(string):\n",
    "    mask = []\n",
    "    for i, seq in enumerate(string):\n",
    "        k = []\n",
    "        for token in seq:\n",
    "            if token == PAD_token:\n",
    "                k.append(PAD_token)\n",
    "            else:\n",
    "                k.append(1)\n",
    "        mask.append(k)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def batchmaker(voc , pair_batch):\n",
    "    input_batch = [] \n",
    "    output_batch = []\n",
    "    for pair in pair_batch:\n",
    "        input_batch.append(pair[0])\n",
    "        output_batch.append(pair[1])\n",
    "    inputs , lens = inps(input_batch , voc)\n",
    "    output , mask , max_l = out(output_batch , voc)\n",
    "    return inputs , lens , output , mask , max_l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9749e0fa-ff43-4212-b222-10addecb6655",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pairs[:batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "42fd5efb-b193-420c-bdd4-81340f32a6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = batchmaker(voc, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "370057ba-1650-422c-a0b7-6d82ee6ac2a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   49,    48,  6417],\n",
       "        [ 1956,    22, 14876],\n",
       "        [    3,  1019,  9984],\n",
       "        [    3,    78,   298],\n",
       "        [ 6278,     3,   257],\n",
       "        [ 4358,     5,     2],\n",
       "        [ 1877,    47,     0],\n",
       "        [    3,     2,     0],\n",
       "        [ 1235,     0,     0],\n",
       "        [  749,     0,     0],\n",
       "        [    3,     0,     0],\n",
       "        [    2,     0,     0]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e545bf-8953-4932-8b30-e2c097104e40",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "443cab7e-0bfe-452c-b4dd-19c73b7a42d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, bidirectional=True)\n",
    "\n",
    "    def forward(self, input_seq, input_lengths, hidden=None):\n",
    "        embedded = self.embedding(input_seq)\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths.cpu(), enforce_sorted=False)\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, :, self.hidden_size:]\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "89b944d0-be32-48f4-84cd-4b7b0d26b694",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "            \n",
    "    def general_score(self, hidden, encoder_output):\n",
    "        energy = self.attn(encoder_output)\n",
    "        return torch.sum(hidden * energy, dim=2)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        if self.method == 'general':\n",
    "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
    "        \n",
    "        attn_energies = attn_energies.t()\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "761f5b45-ba45-4658-8f77-50cd0979cb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self , attn_model, embedding,  hidden_size , output_size , n_layers = 1 , dropout = 0.1):\n",
    "        super(LuongAttnDecoderRNN , self).__init__()\n",
    "        self.attn_model = attn_model \n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size =output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        self.embedding = embedding \n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size , hidden_size , n_layers , dropout = 0 , )\n",
    "        self.concat = nn.Linear(hidden_size*2 , hidden_size)\n",
    "        self.out = nn.Linear(hidden_size , output_size)\n",
    "        self.attn = Attn(attn_model , hidden_size)\n",
    "    def forward(self , input_step , last_hidden , encoder_outputs):\n",
    "        embedded = self.embedding(input_step)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        rnn_output , hidden = self.gru(input_step.int() , last_hidden.int())\n",
    "        attn_weights = self.attn(rnn_output , encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0,1)) #batch matrix-matrix product of matrices\n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        context = context.squeeze(1)\n",
    "        concat_input = torch.cat((rnn_output , context) , 1)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "        output = self.out(concat_output)\n",
    "        output = F.softmax(output , dim = 1)\n",
    "        return output , hidden \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabb14af-3b84-4a8c-97ae-ca262b2edd91",
   "metadata": {},
   "source": [
    "# Testing With previous Batch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9a1592a8-ad1b-4f37-bf1c-2a0b3c22e98e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4850, -0.4314,  0.1394,  ...,  0.3522,  0.2990, -0.5079]],\n",
       "\n",
       "        [[ 0.5007, -0.2831, -0.6359,  ...,  0.3976,  0.7162, -0.7220]],\n",
       "\n",
       "        [[ 0.5200, -0.0534, -0.9816,  ...,  0.0355,  0.3542, -0.9092]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.0643,  0.0987, -0.6060,  ..., -0.0646,  0.2679, -0.3439]],\n",
       "\n",
       "        [[ 0.1153,  0.0197, -0.6474,  ...,  0.2446, -0.1727, -0.7925]],\n",
       "\n",
       "        [[-0.2211,  0.0532,  0.0827,  ..., -0.0730, -0.1277, -0.1193]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = len(voc)\n",
    "src_vocab_size = len(voc)\n",
    "trg_vocab_size = len(voc)\n",
    "hidden_size = 200 \n",
    "\n",
    "encoder = Encoder(input_size, hidden_size)\n",
    "\n",
    "test_input = batches[0]\n",
    "test_input_lengths = torch.tensor([len(test_input)]) \n",
    "\n",
    "# Double checking sequence length:\n",
    "assert test_input.shape[0] >= torch.max(test_input_lengths), \"Declared sequence length exceeds actual length\"\n",
    "\n",
    "# Encoder\n",
    "output, hidden = encoder(test_input, test_input_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f098677-58e0-47b1-abc9-966f6f61b49b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
